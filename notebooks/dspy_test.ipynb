{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/2', creation_time=1751239700003, experiment_id='2', last_update_time=1751239700003, lifecycle_stage='active', name='deep_leads_dspy_test', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import dspy\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env.local\")\n",
    "\n",
    "# Enable autologging with all features\n",
    "mlflow.dspy.autolog(\n",
    "    log_compiles=True,  # Track optimization process\n",
    "    log_evals=True,  # Track evaluation results\n",
    "    log_traces_from_compile=True,  # Track program traces during optimization\n",
    ")\n",
    "\n",
    "# Configure MLflow tracking\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")  # Use local MLflow server\n",
    "mlflow.set_experiment(\"deep_leads_dspy_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program definition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "\n",
    "tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "\n",
    "def browse_web(query: str) -> str:\n",
    "    \"\"\"browse the web for information\"\"\"\n",
    "    try:\n",
    "        research_results = tavily_client.search(query, max_results=5)\n",
    "    except Exception as e:\n",
    "        print(f\"Error browsing the web: {e}\")\n",
    "        return \"Error browsing the web\"\n",
    "\n",
    "    return research_results\n",
    "\n",
    "\n",
    "def get_website_map(url: str) -> str:\n",
    "    \"\"\"get the website map\"\"\"\n",
    "    try:\n",
    "        website_map = tavily_client.map(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting the website map: {e}\")\n",
    "        return \"Error getting the website map\"\n",
    "    return website_map\n",
    "\n",
    "\n",
    "def get_website_content(url: str) -> str:\n",
    "    \"\"\"get the website content\"\"\"\n",
    "    try:\n",
    "        website_content = tavily_client.extract(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting the website content: {e}\")\n",
    "        return \"Error getting the website content\"\n",
    "    return website_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.types import LeadResults\n",
    "\n",
    "# Enable caching\n",
    "dspy.settings.configure(\n",
    "    lm=dspy.LM(\"openai/gpt-4.1\", max_tokens=10000), track_usage=True\n",
    ")\n",
    "\n",
    "\n",
    "class SingleAgentSig(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are an expert lead research agent specializing in finding high-quality contact information for specific professionals,\n",
    "    researchers, and business contacts. Your mission is to conduct thorough, systematic research to identify leads that precisely\n",
    "    match the user's criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    user_query: str = dspy.InputField()\n",
    "    leads: LeadResults = dspy.OutputField()\n",
    "\n",
    "\n",
    "single_agent = dspy.ReAct(\n",
    "    SingleAgentSig,\n",
    "    tools=[browse_web, get_website_map, get_website_content],\n",
    "    max_iters=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from src.agents.utils.build_final_query import build_final_query\n",
    "from src.types import ResearchParams\n",
    "\n",
    "\n",
    "user_query = build_final_query(\n",
    "    ResearchParams(\n",
    "        who_query=\"researchers\",\n",
    "        what_query=\"Human Nutrition\",\n",
    "        where_query=\"Edmonton\",\n",
    "        context_query=\"\",\n",
    "    )\n",
    ")\n",
    "result = await single_agent.acall(user_query=user_query)\n",
    "\n",
    "rprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from src.agents.utils.build_final_query import build_final_query\n",
    "from src.types import ResearchParams\n",
    "\n",
    "dspy.enable_logging()\n",
    "\n",
    "\n",
    "class MultiAgentSig(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are an expert lead research agent specializing in finding high-quality contact information for specific professionals,\n",
    "    researchers, and business contacts. Your mission is to conduct thorough, systematic research to identify leads that precisely\n",
    "    match the user's criteria.\n",
    "\n",
    "    You can use parallel tools calls and deploy a research agent to explore specfic branches of research.\n",
    "    \"\"\"\n",
    "\n",
    "    user_query: str = dspy.InputField()\n",
    "    leads: LeadResults = dspy.OutputField()\n",
    "\n",
    "\n",
    "async def deploy_search_agent(search_query: str) -> LeadResults:\n",
    "    \"\"\"\n",
    "    Deploy a search a research agent that you can use to explore specfic branches of research. This should be used as parallel tool calls.\n",
    "    \"\"\"\n",
    "    return await single_agent.acall(user_query=search_query)\n",
    "\n",
    "\n",
    "multi_agent = dspy.ReAct(\n",
    "    MultiAgentSig,\n",
    "    tools=[browse_web, get_website_map, get_website_content, deploy_search_agent],\n",
    ")\n",
    "\n",
    "user_query = build_final_query(\n",
    "    ResearchParams(\n",
    "        who_query=\"researchers\",\n",
    "        what_query=\"Human Nutrition\",\n",
    "        where_query=\"Edmonton\",\n",
    "        context_query=\"\",\n",
    "    )\n",
    ")\n",
    "result = await multi_agent.acall(user_query=user_query)\n",
    "\n",
    "rprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(result.leads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading and converting trainset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 800 samples\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "from rich import print as rprint\n",
    "from src.types import Sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load and convert the JSON data to Sample objects\n",
    "def load_eval_samples() -> List[Sample]:\n",
    "    \"\"\"Load eval data from JSON and convert to Sample objects\"\"\"\n",
    "    with open(\"checkpoints/eval_leads_v4_800.json\", \"r\") as f:\n",
    "        eval_data = json.load(f)\n",
    "\n",
    "    # Convert each dictionary to a Sample object using Pydantic validation\n",
    "    samples = []\n",
    "    for item in eval_data:\n",
    "        try:\n",
    "            sample = Sample.model_validate(item)\n",
    "            samples.append(sample)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse sample: {e}\")\n",
    "            print(f\"Problematic item: {item}\")\n",
    "            continue\n",
    "\n",
    "    return samples\n",
    "\n",
    "\n",
    "# Load the samples\n",
    "eval_samples = load_eval_samples()\n",
    "print(f\"Successfully loaded {len(eval_samples)} samples\")\n",
    "\n",
    "# Converting data to training and test sets using DSPy Example abstraction\n",
    "\n",
    "\n",
    "def convert_sample_to_dspy_example(sample: Sample) -> dspy.Example:\n",
    "    return dspy.Example(\n",
    "        user_query=sample.query_string,\n",
    "        leads=sample.expected_results.leads,\n",
    "    ).with_inputs(\"user_query\")\n",
    "\n",
    "\n",
    "def train_test_split_list(lst, train_frac=0.8, seed=None):\n",
    "    rng = random.Random(seed)\n",
    "    lst_copy = lst[:]  # shallow copy so original order is preserved\n",
    "    rng.shuffle(lst_copy)\n",
    "    split = int(len(lst_copy) * train_frac)\n",
    "    return lst_copy[:split], lst_copy[split:]\n",
    "\n",
    "\n",
    "def get_train_test_split(\n",
    "    samples: List[Sample], test_size: float = 0.8\n",
    ") -> Tuple[List[dspy.Example], List[dspy.Example]]:\n",
    "    examples = [convert_sample_to_dspy_example(sample) for sample in samples]\n",
    "    train_examples, test_examples = train_test_split(examples, test_size=test_size)\n",
    "    return train_examples, test_examples\n",
    "\n",
    "\n",
    "train_examples, test_examples = get_train_test_split(eval_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def cosine_similarity_matrix(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute pairwise cosine similarities between rows of A and rows of B\n",
    "    using SciPy's cdist (which returns cosine distances = 1 - cosine_similarity).\n",
    "    \"\"\"\n",
    "    # if either is empty, return an empty (M, N) matrix\n",
    "    if A.size == 0 or B.size == 0:\n",
    "        return np.zeros((A.shape[0], B.shape[0]))\n",
    "\n",
    "    cos_dist = cdist(A, B, metric=\"cosine\")  # shape (M, N)\n",
    "    return 1.0 - cos_dist  # convert distance -> similarity\n",
    "\n",
    "\n",
    "def score_sample(\n",
    "    true_names: list[str], pred_names: list[str], threshold: float = 0.7\n",
    ") -> tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    For one sample (one list of true names, one list of predicted names):\n",
    "      - embed both sets,\n",
    "      - build similarity matrix,\n",
    "      - run Hungarian to match,\n",
    "      - count TP, FP, FN at the given sim threshold.\n",
    "    Returns (TP, FP, FN).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. embed\n",
    "    embedder = dspy.Embedder(\"openai/text-embedding-3-small\", batch_size=100)\n",
    "    E_true = embedder(true_names)  # shape (M, D)\n",
    "    E_pred = embedder(pred_names)  # shape (N, D)\n",
    "\n",
    "    # 2. similarity matrix\n",
    "    sim = cosine_similarity_matrix(E_true, E_pred)  # (M, N)\n",
    "\n",
    "    # 3. Hungarian matching on negative sim to maximize total similarity\n",
    "    #    If Mâ‰ N, linear_sum_assignment will match min(M,N) pairs.\n",
    "    row_idx, col_idx = linear_sum_assignment(-sim)\n",
    "\n",
    "    # 4. filter by threshold\n",
    "    matched_sims = sim[row_idx, col_idx]\n",
    "    good = matched_sims >= threshold\n",
    "\n",
    "    TP = int(good.sum())\n",
    "    FP = len(pred_names) - TP\n",
    "    FN = len(true_names) - TP\n",
    "\n",
    "    # Precision: Of all predicted leads, how many were correct?\n",
    "    precision = (TP / (TP + FP)) if (TP + FP) > 0 else 0.0\n",
    "\n",
    "    # Recall: Of all expected leads, how many were found?\n",
    "    recall = (TP / (TP + FN)) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "    # F1 Score: Harmonic mean of precision and recall\n",
    "    f1_score = (\n",
    "        (2 * precision * recall) / (precision + recall)\n",
    "        if (precision + recall) > 0\n",
    "        else 0.0\n",
    "    )\n",
    "\n",
    "    return recall, precision, f1_score\n",
    "\n",
    "\n",
    "def validate_leads_recall(\n",
    "    example: dspy.Example, pred: dspy.Prediction, trace=None\n",
    ") -> float:\n",
    "    true_names = [lead.name for lead in example.leads]\n",
    "    pred_names = [lead.name for lead in pred.leads.leads]\n",
    "\n",
    "    recall, _, _ = score_sample(true_names, pred_names)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def validate_leads_precision(\n",
    "    example: dspy.Example, pred: dspy.Prediction, trace=None\n",
    ") -> float:\n",
    "    true_names = [lead.name for lead in example.leads]\n",
    "    pred_names = [lead.name for lead in pred.leads.leads]\n",
    "\n",
    "    _, precision, _ = score_sample(true_names, pred_names)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def validate_leads_f1_score(\n",
    "    example: dspy.Example, pred: dspy.Prediction, trace=None\n",
    ") -> float:\n",
    "    true_names = [lead.name for lead in example.leads]\n",
    "    pred_names = [lead.name for lead in pred.leads.leads]\n",
    "\n",
    "    _, _, f1_score = score_sample(true_names, pred_names)\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valuation Executors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def print_eval_results(\n",
    "    scores, tavily_cost, llm_cost, total_cost, dataset_size, num_threads=None\n",
    "):\n",
    "    \"\"\"Print nicely formatted evaluation results\"\"\"\n",
    "    from rich.console import Console\n",
    "    from rich.table import Table\n",
    "    from rich.panel import Panel\n",
    "    from rich.text import Text\n",
    "\n",
    "    console = Console()\n",
    "\n",
    "    # Calculate statistics\n",
    "    average_score = sum(scores) / len(scores) if scores else 0\n",
    "    max_score = max(scores) if scores else 0\n",
    "    min_score = min(scores) if scores else 0\n",
    "    success_rate = (\n",
    "        len([s for s in scores if s > 0]) / len(scores) * 100 if scores else 0\n",
    "    )\n",
    "\n",
    "    # Create main results table\n",
    "    table = Table(title=\"ðŸ” Evaluation Results\", title_style=\"bold blue\")\n",
    "    table.add_column(\"Metric\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Value\", style=\"magenta\")\n",
    "    table.add_column(\"Details\", style=\"green\")\n",
    "\n",
    "    # Performance metrics\n",
    "    table.add_row(\n",
    "        \"ðŸ“Š Average Score\",\n",
    "        f\"{average_score:.4f}\",\n",
    "        f\"Range: {min_score:.4f} - {max_score:.4f}\",\n",
    "    )\n",
    "    table.add_row(\n",
    "        \"âœ… Success Rate\",\n",
    "        f\"{success_rate:.1f}%\",\n",
    "        f\"{len([s for s in scores if s > 0])}/{len(scores)} samples\",\n",
    "    )\n",
    "    table.add_row(\"ðŸ“ˆ Total Samples\", f\"{dataset_size}\", f\"Completed: {len(scores)}\")\n",
    "\n",
    "    if num_threads:\n",
    "        table.add_row(\n",
    "            \"ðŸ”„ Parallelization\", f\"{num_threads} threads\", \"Concurrent execution\"\n",
    "        )\n",
    "\n",
    "    # Cost breakdown\n",
    "    table.add_row(\"\", \"\", \"\")  # Separator\n",
    "    table.add_row(\n",
    "        \"ðŸ’° Tavily Cost\", f\"${tavily_cost:.4f}\", f\"{int(tavily_cost / 0.008)} API calls\"\n",
    "    )\n",
    "    table.add_row(\"ðŸ¤– LLM Cost\", f\"${llm_cost:.4f}\", \"Language model usage\")\n",
    "    table.add_row(\"ðŸ’¸ Total Cost\", f\"${total_cost:.4f}\", \"All services combined\")\n",
    "\n",
    "    # Cost per sample\n",
    "    cost_per_sample = total_cost / len(scores) if scores else 0\n",
    "    table.add_row(\n",
    "        \"ðŸ“‹ Cost/Sample\", f\"${cost_per_sample:.4f}\", \"Average cost per evaluation\"\n",
    "    )\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "    # Score distribution\n",
    "    if scores:\n",
    "        score_text = Text()\n",
    "        score_text.append(\"Score Distribution: \", style=\"bold\")\n",
    "\n",
    "        # Create simple histogram\n",
    "        score_ranges = [\n",
    "            (0.0, 0.2, \"ðŸ”´\"),\n",
    "            (0.2, 0.4, \"ðŸŸ \"),\n",
    "            (0.4, 0.6, \"ðŸŸ¡\"),\n",
    "            (0.6, 0.8, \"ðŸŸ¢\"),\n",
    "            (0.8, 1.0, \"ðŸŸ¦\"),\n",
    "        ]\n",
    "\n",
    "        for low, high, emoji in score_ranges:\n",
    "            count = len(\n",
    "                [s for s in scores if low <= s < high or (high == 1.0 and s == 1.0)]\n",
    "            )\n",
    "            if count > 0:\n",
    "                score_text.append(\n",
    "                    f\"{emoji} {low:.1f}-{high:.1f}: {count} \", style=\"white\"\n",
    "                )\n",
    "\n",
    "        console.print(\n",
    "            Panel(score_text, title=\"ðŸ“Š Score Breakdown\", border_style=\"blue\")\n",
    "        )\n",
    "\n",
    "\n",
    "# Alternative simpler version without rich dependency\n",
    "def print_eval_results_simple(\n",
    "    scores, tavily_cost, llm_cost, total_cost, dataset_size, num_threads=None\n",
    "):\n",
    "    \"\"\"Print nicely formatted evaluation results (no rich dependency)\"\"\"\n",
    "\n",
    "    # Calculate statistics\n",
    "    average_score = sum(scores) / len(scores) if scores else 0\n",
    "    max_score = max(scores) if scores else 0\n",
    "    min_score = min(scores) if scores else 0\n",
    "    success_rate = (\n",
    "        len([s for s in scores if s > 0]) / len(scores) * 100 if scores else 0\n",
    "    )\n",
    "    cost_per_sample = total_cost / len(scores) if scores else 0\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸ” EVALUATION RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"ðŸ“Š PERFORMANCE METRICS\")\n",
    "    print(f\"   Average Score:     {average_score:.4f}\")\n",
    "    print(f\"   Score Range:       {min_score:.4f} - {max_score:.4f}\")\n",
    "    print(\n",
    "        f\"   Success Rate:      {success_rate:.1f}% ({len([s for s in scores if s > 0])}/{len(scores)} samples)\"\n",
    "    )\n",
    "    print(f\"   Total Samples:     {dataset_size}\")\n",
    "    if num_threads:\n",
    "        print(f\"   Threads Used:      {num_threads}\")\n",
    "\n",
    "    print(\"\\nðŸ’° COST BREAKDOWN\")\n",
    "    print(\n",
    "        f\"   Tavily API:        ${tavily_cost:.4f} ({int(tavily_cost / 0.008)} calls)\"\n",
    "    )\n",
    "    print(f\"   LLM Usage:         ${llm_cost:.4f}\")\n",
    "    print(f\"   Total Cost:        ${total_cost:.4f}\")\n",
    "    print(f\"   Cost per Sample:   ${cost_per_sample:.4f}\")\n",
    "\n",
    "    print(\"\\nðŸ“Š SCORE DISTRIBUTION\")\n",
    "    score_ranges = [\n",
    "        (0.0, 0.2, \"Poor\"),\n",
    "        (0.2, 0.4, \"Fair\"),\n",
    "        (0.4, 0.6, \"Good\"),\n",
    "        (0.6, 0.8, \"Very Good\"),\n",
    "        (0.8, 1.0, \"Excellent\"),\n",
    "    ]\n",
    "    for low, high, label in score_ranges:\n",
    "        count = len(\n",
    "            [s for s in scores if low <= s < high or (high == 1.0 and s == 1.0)]\n",
    "        )\n",
    "        if count > 0:\n",
    "            print(f\"   {label:10} ({low:.1f}-{high:.1f}): {count:3d} samples\")\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def get_tavily_usage(previous_usage: int = 0):\n",
    "    url = \"https://api.tavily.com/usage\"\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('TAVILY_API_KEY')}\"}\n",
    "\n",
    "    try:\n",
    "        response = requests.request(\"GET\", url, headers=headers)\n",
    "        last_usage = response.json()[\"key\"][\"usage\"]\n",
    "        return last_usage - previous_usage\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting Tavily usage: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_llm_cost(program):\n",
    "    return program.history[-1][\"cost\"]\n",
    "\n",
    "\n",
    "def run_evals_sequentially(\n",
    "    model, dataset, metric, program, delay_seconds=1, experiment_name=\"Test_eval_1\"\n",
    "):\n",
    "    tavily_start_usage = get_tavily_usage()\n",
    "    if tavily_start_usage is None:\n",
    "        time.sleep(60)\n",
    "        tavily_start_usage = get_tavily_usage()\n",
    "\n",
    "    mlflow.dspy.autolog(log_traces_from_eval=True)\n",
    "\n",
    "    with mlflow.start_run(run_name=experiment_name):\n",
    "        scores = []\n",
    "        llm_cost = 0\n",
    "        average_score = 0\n",
    "        for sample in tqdm(dataset):\n",
    "            time.sleep(delay_seconds)\n",
    "            with dspy.context(lm=dspy.LM(model, max_tokens=10000)):\n",
    "                pred = program(**sample.inputs())\n",
    "            llm_cost += get_llm_cost(program)\n",
    "            score = metric(sample, pred)\n",
    "            scores.append(score)\n",
    "            average_score = sum(scores) / len(scores)\n",
    "            print(f\"Latest score: {score}, average score: {average_score}\")\n",
    "\n",
    "            # Log the aggregated score\n",
    "            mlflow.log_metric(\"Recall_average_score\", average_score)\n",
    "\n",
    "    tavily_usage_end_of_run = get_tavily_usage()\n",
    "    tavily_usage = tavily_usage_end_of_run - tavily_start_usage\n",
    "    tavily_cost = tavily_usage * 0.008\n",
    "    total_cost = tavily_cost + llm_cost\n",
    "\n",
    "    print_eval_results(scores, tavily_cost, llm_cost, total_cost, len(dataset))\n",
    "\n",
    "    return scores, tavily_cost, llm_cost, total_cost\n",
    "\n",
    "\n",
    "def run_evals_parallel(\n",
    "    model,\n",
    "    dataset,\n",
    "    metric,\n",
    "    program,\n",
    "    num_threads=4,\n",
    "    delay_seconds=3,\n",
    "    experiment_name=\"Test_eval_1\",\n",
    "):\n",
    "    scores = []\n",
    "    scores_lock = threading.Lock()\n",
    "\n",
    "    # Cost tracking variables\n",
    "    tavily_start_usage = get_tavily_usage()\n",
    "    if tavily_start_usage is None:\n",
    "        time.sleep(60)\n",
    "        tavily_start_usage = get_tavily_usage()\n",
    "\n",
    "    llm_cost = 0\n",
    "    cost_lock = threading.Lock()\n",
    "\n",
    "    def evaluate_sample(sample):\n",
    "        nonlocal llm_cost\n",
    "\n",
    "        time.sleep(delay_seconds)\n",
    "        with dspy.context(lm=dspy.LM(model, max_tokens=10000)):\n",
    "            pred = program(**sample.inputs())\n",
    "\n",
    "        # Get costs for this sample\n",
    "        sample_llm_cost = get_llm_cost(program)\n",
    "\n",
    "        score = metric(sample, pred)\n",
    "\n",
    "        # Thread-safe updates\n",
    "        with cost_lock:\n",
    "            llm_cost += sample_llm_cost\n",
    "\n",
    "        with scores_lock:\n",
    "            scores.append(score)\n",
    "            average_score = sum(scores) / len(scores)\n",
    "            print(\n",
    "                f\"Latest score: {score}, average score: {average_score:.4f}, completed: {len(scores)}/{len(dataset)}\"\n",
    "            )\n",
    "\n",
    "        return score\n",
    "\n",
    "    mlflow.dspy.autolog(log_traces_from_eval=True)\n",
    "    with mlflow.start_run(run_name=experiment_name):\n",
    "        with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_sample = {\n",
    "                executor.submit(evaluate_sample, sample): sample for sample in dataset\n",
    "            }\n",
    "\n",
    "            # Process completed tasks\n",
    "            for future in tqdm(\n",
    "                as_completed(future_to_sample), total=len(future_to_sample)\n",
    "            ):\n",
    "                try:\n",
    "                    future.result()  # This will raise any exceptions that occurred\n",
    "                except Exception as e:\n",
    "                    sample = future_to_sample[future]\n",
    "                    print(f\"Error evaluating sample: {e}\")\n",
    "\n",
    "        # Calculate final costs and average scores\n",
    "        mlflow.log_metric(\"Recall_average_score\", sum(scores) / len(scores))\n",
    "\n",
    "    tavily_usage_end_of_run = get_tavily_usage()\n",
    "    if tavily_usage_end_of_run is None:\n",
    "        sleep_time = 60 * 4\n",
    "        print(f\"Tavily usage is None, waiting {sleep_time} seconds\")\n",
    "        time.sleep(sleep_time)\n",
    "        tavily_usage_end_of_run = get_tavily_usage()\n",
    "    tavily_usage = tavily_usage_end_of_run - tavily_start_usage\n",
    "    tavily_cost = tavily_usage * 0.008\n",
    "    total_cost = tavily_cost + llm_cost\n",
    "\n",
    "    print_eval_results(scores, tavily_cost, llm_cost, total_cost, len(dataset))\n",
    "\n",
    "    return scores, tavily_cost, llm_cost, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Runs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Single Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error getting the website map: 422 Client Error: Unprocessable Entity for url: https://api.tavily.com/map\n",
      "Error getting the website content: Your request has been blocked due to excessive requests. Please reduce the rate of requests.\n",
      "Error getting the website map: 422 Client Error: Unprocessable Entity for url: https://api.tavily.com/map\n",
      "Error getting the website map: 422 Client Error: Unprocessable Entity for url: https://api.tavily.com/map\n",
      "Error browsing the web: Your request has been blocked due to excessive requests. Please reduce the rate of requests.\n",
      "Error browsing the web: Your request has been blocked due to excessive requests. Please reduce the rate of requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [01:12<02:24, 72.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest score: 0.07462686567164178, average score: 0.0746, completed: 1/3\n",
      "Error browsing the web: Your request has been blocked due to excessive requests. Please reduce the rate of requests.\n",
      "Error getting the website content: Your request has been blocked due to excessive requests. Please reduce the rate of requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:28<00:39, 39.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest score: 0.6666666666666666, average score: 0.3706, completed: 2/3\n",
      "Error browsing the web: Your request has been blocked due to excessive requests. Please reduce the rate of requests.\n",
      "Error browsing the web: Your request has been blocked due to excessive requests. Please reduce the rate of requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:48<00:00, 36.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest score: 0.0, average score: 0.2471, completed: 3/3\n",
      "ðŸƒ View run Test_eval_1 at: http://127.0.0.1:5000/#/experiments/2/runs/5ff81073a25b4b9eaba563944db1406d\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mrun_evals_parallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenai/gpt-4.1-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_examples\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_leads_recall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m rprint(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 271\u001b[39m, in \u001b[36mrun_evals_parallel\u001b[39m\u001b[34m(model, dataset, metric, program, num_threads, delay_seconds, experiment_name)\u001b[39m\n\u001b[32m    268\u001b[39m                 \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError evaluating sample: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m     \u001b[38;5;66;03m# Calculate final costs and average score\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRecall_all_scores\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    272\u001b[39m     mlflow.log_metric(\u001b[33m\"\u001b[39m\u001b[33mRecall_average_score\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m(scores) / \u001b[38;5;28mlen\u001b[39m(scores))\n\u001b[32m    274\u001b[39m tavily_usage_end_of_run = get_tavily_usage()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/deep_leads/.venv/lib/python3.11/site-packages/mlflow/tracking/fluent.py:981\u001b[39m, in \u001b[36mlog_metric\u001b[39m\u001b[34m(key, value, step, synchronous, timestamp, run_id, model_id, dataset)\u001b[39m\n\u001b[32m    975\u001b[39m model_ids = (\n\u001b[32m    976\u001b[39m     [model_id]\n\u001b[32m    977\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m (_get_model_ids_for_new_metric_if_exist(run_id, step) \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[32m    979\u001b[39m )\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_id \u001b[38;5;129;01min\u001b[39;00m model_ids:\n\u001b[32m--> \u001b[39m\u001b[32m981\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_digest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdigest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/deep_leads/.venv/lib/python3.11/site-packages/mlflow/tracking/client.py:2028\u001b[39m, in \u001b[36mMlflowClient.log_metric\u001b[39m\u001b[34m(self, run_id, key, value, timestamp, step, synchronous, dataset_name, dataset_digest, model_id)\u001b[39m\n\u001b[32m   2024\u001b[39m synchronous = (\n\u001b[32m   2025\u001b[39m     synchronous \u001b[38;5;28;01mif\u001b[39;00m synchronous \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING.get()\n\u001b[32m   2026\u001b[39m )\n\u001b[32m   2027\u001b[39m model_id = model_id \u001b[38;5;129;01mor\u001b[39;00m get_active_model_id()\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_metric\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2034\u001b[39m \u001b[43m    \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynchronous\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2036\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_digest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset_digest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2038\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/deep_leads/.venv/lib/python3.11/site-packages/mlflow/tracking/_tracking_service/client.py:350\u001b[39m, in \u001b[36mTrackingServiceClient.log_metric\u001b[39m\u001b[34m(self, run_id, key, value, timestamp, step, synchronous, dataset_name, dataset_digest, model_id)\u001b[39m\n\u001b[32m    348\u001b[39m timestamp = timestamp \u001b[38;5;28;01mif\u001b[39;00m timestamp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m get_current_time_millis()\n\u001b[32m    349\u001b[39m step = step \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m metric_value = \u001b[43mconvert_metric_value_to_float_if_possible\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m metric = Metric(\n\u001b[32m    352\u001b[39m     key,\n\u001b[32m    353\u001b[39m     metric_value,\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m     dataset_digest=dataset_digest,\n\u001b[32m    359\u001b[39m )\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/deep_leads/.venv/lib/python3.11/site-packages/mlflow/tracking/metric_value_conversion_utils.py:53\u001b[39m, in \u001b[36mconvert_metric_value_to_float_if_possible\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     50\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m possible_float\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mTypeError\u001b[39m: float() argument must be a string or a real number, not 'list'"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://127.0.0.1:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=ccb9c843524841a9b7f635e73cca0dea&amp;experiment_id=2&amp;trace_id=09026d3739eb45238a5cc026f0910212&amp;experiment_id=2&amp;trace_id=606e10f7983f40a79505d1ec846b9d0a&amp;experiment_id=2&amp;version=3.1.1\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(trace_id=ccb9c843524841a9b7f635e73cca0dea), Trace(trace_id=09026d3739eb45238a5cc026f0910212), Trace(trace_id=606e10f7983f40a79505d1ec846b9d0a)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = run_evals_parallel(\n",
    "    model=\"openai/gpt-4.1-mini\",\n",
    "    dataset=test_examples[:3],\n",
    "    metric=validate_leads_recall,\n",
    "    program=single_agent,\n",
    ")\n",
    "\n",
    "rprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import Evaluate\n",
    "\n",
    "mlflow.dspy.autolog(log_traces_from_eval=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"Test_eval_1\"):\n",
    "    # Set up the evaluator, which can be re-used in your code.\n",
    "    evaluator = Evaluate(\n",
    "        devset=test_examples[0:],\n",
    "        num_threads=3,\n",
    "        display_progress=True,\n",
    "        display_table=5,\n",
    "        show_progress=True,\n",
    "        provide_traceback=True,\n",
    "        return_all_scores=True,\n",
    "        return_outputs=True,\n",
    "    )\n",
    "\n",
    "    aggregated_score, outputs, all_scores = evaluator(\n",
    "        single_agent, metric=validate_leads_recall\n",
    "    )\n",
    "    # Log the aggregated score\n",
    "    mlflow.log_metric(\"exact_match\", aggregated_score)\n",
    "    # Log the detailed evaluation results as a table\n",
    "    mlflow.log_table(\n",
    "        {\n",
    "            \"question\": [example.user_query for example in test_examples[1:4]],\n",
    "            \"answer\": [example.leads for example in test_examples[1:4]],\n",
    "            \"output\": outputs,\n",
    "            \"recall\": all_scores,\n",
    "        },\n",
    "        artifact_file=\"eval_results.json\",\n",
    "    )\n",
    "\n",
    "# Launch evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import TYPE_CHECKING, Any, Callable, Type, List\n",
    "\n",
    "from litellm import ContextWindowExceededError\n",
    "\n",
    "import dspy\n",
    "from dspy.adapters.types.tool import Tool\n",
    "from dspy.signatures.signature import ensure_signature\n",
    "from rich import print as rprint\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from dspy.signatures.signature import Signature\n",
    "\n",
    "\n",
    "class ReAct(dspy.Module):\n",
    "    def __init__(\n",
    "        self, signature: Type[\"Signature\"], tools: list[Callable], max_iters: int = 10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ReAct stands for \"Reasoning and Acting,\" a popular paradigm for building tool-using agents.\n",
    "        In this approach, the language model is iteratively provided with a list of tools and has\n",
    "        to reason about the current situation. The model decides whether to call a tool to gather more\n",
    "        information or to finish the task based on its reasoning process. The DSPy version of ReAct is\n",
    "        generalized to work over any signature, thanks to signature polymorphism.\n",
    "\n",
    "        Args:\n",
    "            signature: The signature of the module, which defines the input and output of the react module.\n",
    "            tools (list[Callable]): A list of functions, callable objects, or `dspy.Tool` instances.\n",
    "            max_iters (Optional[int]): The maximum number of iterations to run. Defaults to 10.\n",
    "\n",
    "        Example:\n",
    "\n",
    "        ```python\n",
    "        def get_weather(city: str) -> str:\n",
    "            return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "        react = dspy.ReAct(signature=\"question->answer\", tools=[get_weather])\n",
    "        pred = react(question=\"What is the weather in Tokyo?\")\n",
    "        ```\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.signature = signature = ensure_signature(signature)\n",
    "        self.max_iters = max_iters\n",
    "\n",
    "        tools = [t if isinstance(t, Tool) else Tool(t) for t in tools]\n",
    "        tools = {tool.name: tool for tool in tools}\n",
    "\n",
    "        inputs = \", \".join([f\"`{k}`\" for k in signature.input_fields.keys()])\n",
    "        outputs = \", \".join([f\"`{k}`\" for k in signature.output_fields.keys()])\n",
    "        instr = [f\"{signature.instructions}\\n\"] if signature.instructions else []\n",
    "\n",
    "        instr.extend(\n",
    "            [\n",
    "                f\"You are an Agent. In each episode, you will be given the fields {inputs} as input. And you can see your past trajectory so far.\",\n",
    "                f\"Your goal is to use one or more of the supplied tools to collect any necessary information for producing {outputs}.\\n\",\n",
    "                \"To do this, you will interleave next_thought, next_tool_name, and next_tool_args in each turn, and also when finishing the task.\",\n",
    "                \"You can use multiple tools in each turn, and you can use the same tool multiple times in the same turn.\",\n",
    "                \"After each tool call, you receive a resulting observation, which gets appended to your trajectory.\\n\",\n",
    "                \"When writing next_thought, you may reason about the current situation and plan for future steps.\",\n",
    "                \"When selecting next_tool_name and its next_tool_args, the tools must be on the following list:\\n\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tools[\"finish\"] = Tool(\n",
    "            func=lambda: \"Completed.\",\n",
    "            name=\"finish\",\n",
    "            desc=f\"Marks the task as complete. That is, signals that all information for producing the outputs, i.e. {outputs}, are now available to be extracted.\",\n",
    "            args={},\n",
    "        )\n",
    "\n",
    "        for idx, tool in enumerate(tools.values()):\n",
    "            instr.append(f\"({idx + 1}) {tool}\")\n",
    "        instr.append(\n",
    "            \"When providing `next_tool_args`, the value inside the field must be in JSON format\"\n",
    "        )\n",
    "\n",
    "        react_signature = (\n",
    "            dspy.Signature({**signature.input_fields}, \"\\n\".join(instr))\n",
    "            .append(\"trajectory\", dspy.InputField(), type_=str)\n",
    "            .append(\"next_thought\", dspy.OutputField(), type_=str)\n",
    "            .append(\n",
    "                \"next_tool_name\",\n",
    "                dspy.OutputField(),\n",
    "                type_=List[Literal[tuple(tools.keys())]],\n",
    "            )\n",
    "            .append(\"next_tool_args\", dspy.OutputField(), type_=List[dict[str, Any]])\n",
    "        )\n",
    "\n",
    "        fallback_signature = dspy.Signature(\n",
    "            {**signature.input_fields, **signature.output_fields},\n",
    "            signature.instructions,\n",
    "        ).append(\"trajectory\", dspy.InputField(), type_=str)\n",
    "\n",
    "        self.tools = tools\n",
    "        self.react = dspy.Predict(react_signature)\n",
    "        self.extract = dspy.ChainOfThought(fallback_signature)\n",
    "\n",
    "    def _format_trajectory(self, trajectory: dict[str, Any]):\n",
    "        adapter = dspy.settings.adapter or dspy.ChatAdapter()\n",
    "        trajectory_signature = dspy.Signature(f\"{', '.join(trajectory.keys())} -> x\")\n",
    "        return adapter.format_user_message_content(trajectory_signature, trajectory)\n",
    "\n",
    "    def forward(self, **input_args):\n",
    "        trajectory = {}\n",
    "        max_iters = input_args.pop(\"max_iters\", self.max_iters)\n",
    "        for idx in range(max_iters):\n",
    "            try:\n",
    "                pred = self._call_with_potential_trajectory_truncation(\n",
    "                    self.react, trajectory, **input_args\n",
    "                )\n",
    "            except ValueError as err:\n",
    "                logger.warning(\n",
    "                    f\"Ending the trajectory: Agent failed to select a valid tool: {_fmt_exc(err)}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "            rprint(pred)\n",
    "\n",
    "            trajectory[f\"thought_{idx}\"] = pred.next_thought\n",
    "            trajectory[f\"tool_name_{idx}\"] = pred.next_tool_name\n",
    "            trajectory[f\"tool_args_{idx}\"] = pred.next_tool_args\n",
    "\n",
    "            try:\n",
    "                trajectory[f\"observation_{idx}\"] = self.tools[pred.next_tool_name](\n",
    "                    **pred.next_tool_args\n",
    "                )\n",
    "            except Exception as err:\n",
    "                trajectory[f\"observation_{idx}\"] = (\n",
    "                    f\"Execution error in {pred.next_tool_name}: {_fmt_exc(err)}\"\n",
    "                )\n",
    "\n",
    "            if pred.next_tool_name == \"finish\":\n",
    "                break\n",
    "\n",
    "        extract = self._call_with_potential_trajectory_truncation(\n",
    "            self.extract, trajectory, **input_args\n",
    "        )\n",
    "        return dspy.Prediction(trajectory=trajectory, **extract)\n",
    "\n",
    "    async def aforward(self, **input_args):\n",
    "        trajectory = {}\n",
    "        max_iters = input_args.pop(\"max_iters\", self.max_iters)\n",
    "        for idx in range(max_iters):\n",
    "            try:\n",
    "                pred = await self._async_call_with_potential_trajectory_truncation(\n",
    "                    self.react, trajectory, **input_args\n",
    "                )\n",
    "            except ValueError as err:\n",
    "                logger.warning(\n",
    "                    f\"Ending the trajectory: Agent failed to select a valid tool: {_fmt_exc(err)}\"\n",
    "                )\n",
    "                break\n",
    "\n",
    "            print(\"printing pred\")\n",
    "            rprint(pred)\n",
    "            print()\n",
    "\n",
    "            trajectory[f\"thought_{idx}\"] = pred.next_thought\n",
    "            trajectory[f\"tool_name_{idx}\"] = pred.next_tool_name\n",
    "            trajectory[f\"tool_args_{idx}\"] = pred.next_tool_args\n",
    "\n",
    "            try:\n",
    "                trajectory[f\"observation_{idx}\"] = await self.tools[\n",
    "                    pred.next_tool_name\n",
    "                ].acall(**pred.next_tool_args)\n",
    "            except Exception as err:\n",
    "                trajectory[f\"observation_{idx}\"] = (\n",
    "                    f\"Execution error in {pred.next_tool_name}: {_fmt_exc(err)}\"\n",
    "                )\n",
    "\n",
    "            if pred.next_tool_name == \"finish\":\n",
    "                break\n",
    "\n",
    "        extract = await self._async_call_with_potential_trajectory_truncation(\n",
    "            self.extract, trajectory, **input_args\n",
    "        )\n",
    "        return dspy.Prediction(trajectory=trajectory, **extract)\n",
    "\n",
    "    def _call_with_potential_trajectory_truncation(\n",
    "        self, module, trajectory, **input_args\n",
    "    ):\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                return module(\n",
    "                    **input_args,\n",
    "                    trajectory=self._format_trajectory(trajectory),\n",
    "                )\n",
    "            except ContextWindowExceededError:\n",
    "                logger.warning(\n",
    "                    \"Trajectory exceeded the context window, truncating the oldest tool call information.\"\n",
    "                )\n",
    "                trajectory = self.truncate_trajectory(trajectory)\n",
    "\n",
    "    async def _async_call_with_potential_trajectory_truncation(\n",
    "        self, module, trajectory, **input_args\n",
    "    ):\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                return await module.acall(\n",
    "                    **input_args,\n",
    "                    trajectory=self._format_trajectory(trajectory),\n",
    "                )\n",
    "            except ContextWindowExceededError:\n",
    "                logger.warning(\n",
    "                    \"Trajectory exceeded the context window, truncating the oldest tool call information.\"\n",
    "                )\n",
    "                trajectory = self.truncate_trajectory(trajectory)\n",
    "\n",
    "    def truncate_trajectory(self, trajectory):\n",
    "        \"\"\"Truncates the trajectory so that it fits in the context window.\n",
    "\n",
    "        Users can override this method to implement their own truncation logic.\n",
    "        \"\"\"\n",
    "        keys = list(trajectory.keys())\n",
    "        if len(keys) < 4:\n",
    "            # Every tool call has 4 keys: thought, tool_name, tool_args, and observation.\n",
    "            raise ValueError(\n",
    "                \"The trajectory is too long so your prompt exceeded the context window, but the trajectory cannot be \"\n",
    "                \"truncated because it only has one tool call.\"\n",
    "            )\n",
    "\n",
    "        for key in keys[:4]:\n",
    "            trajectory.pop(key)\n",
    "\n",
    "        return trajectory\n",
    "\n",
    "\n",
    "def _fmt_exc(err: BaseException, *, limit: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Return a one-string traceback summary.\n",
    "    * `limit` - how many stack frames to keep (from the innermost outwards).\n",
    "    \"\"\"\n",
    "\n",
    "    import traceback\n",
    "\n",
    "    return (\n",
    "        \"\\n\"\n",
    "        + \"\".join(\n",
    "            traceback.format_exception(type(err), err, err.__traceback__, limit=limit)\n",
    "        ).strip()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.types import LeadResults\n",
    "\n",
    "# Enable caching\n",
    "dspy.settings.configure(lm=dspy.LM(\"openai/gpt-4.1\"), track_usage=True)\n",
    "\n",
    "\n",
    "class SingleAgentSig(dspy.Signature):\n",
    "    \"\"\"\n",
    "    You are an expert lead research agent specializing in finding high-quality contact information for specific professionals,\n",
    "    researchers, and business contacts. Your mission is to conduct thorough, systematic research to identify leads that precisely\n",
    "    match the user's criteria.\n",
    "\n",
    "    You can use multiple tools in each turn, having them ran in parallel on the same search.\n",
    "    You can also use the same tool multiple times in the same turn.\n",
    "    \"\"\"\n",
    "\n",
    "    user_query: str = dspy.InputField()\n",
    "    leads: LeadResults = dspy.OutputField()\n",
    "\n",
    "\n",
    "single_agent = ReAct(\n",
    "    SingleAgentSig, tools=[browse_web, get_website_map, get_website_content]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print as rprint\n",
    "from src.agents.utils.build_final_query import build_final_query\n",
    "from src.types import ResearchParams\n",
    "\n",
    "\n",
    "user_query = build_final_query(\n",
    "    ResearchParams(\n",
    "        who_query=\"researchers\",\n",
    "        what_query=\"Human Nutrition\",\n",
    "        where_query=\"Edmonton\",\n",
    "        context_query=\"\",\n",
    "    )\n",
    ")\n",
    "result = await single_agent.acall(user_query=user_query)\n",
    "\n",
    "rprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
