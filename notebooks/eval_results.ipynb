{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single agent version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.human_verified_searches import (\n",
    "    EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    ")\n",
    "from src.evals.eval_runner import test_correctness_with_visual_comparison\n",
    "from src.agents.single_agent_pattern import run_single_agent\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "results = await test_correctness_with_visual_comparison(\n",
    "    call_agent=run_single_agent,\n",
    "    eval_params=EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    "    n_results_search=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.human_verified_searches import EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE\n",
    "from src.evals.eval_runner import test_correctness_with_visual_comparison\n",
    "from src.agents.single_agent_pattern import run_single_agent\n",
    "from rich import print as rprint\n",
    "\n",
    "\n",
    "results = await test_correctness_with_visual_comparison(\n",
    "    call_agent=run_single_agent,\n",
    "    eval_params=EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE,\n",
    "    n_results_search=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "\n",
    "- [ ] Models allucinate a lot of information on the lack more details (example: sees a name on a article but can't find the person inforamtion, it then allucinate their contact info)\n",
    "- [ ] The search paths are very narrow and and can totally get out of path when faced with some timeout or other types of page retrieval errors.\n",
    "- [ ] Even though many parallel calls are made the explorations paths are still not enough to cover possibilities.\n",
    "- [ ] There is an important need to have a fact check phase. The model tends to ignore important filtering cases like WHERE and CONTEXT clausese. This normally happens when models expand search and then the filters get \"lost\" in their context or attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi agent version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.human_verified_searches import (\n",
    "    EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    ")\n",
    "from src.evals.eval_runner import test_correctness_with_visual_comparison\n",
    "from src.agents.multi_agent_pattern import run_multi_agent\n",
    "\n",
    "\n",
    "results = await test_correctness_with_visual_comparison(\n",
    "    call_agent=run_multi_agent,\n",
    "    eval_params=EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    "    n_results_search=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.human_verified_searches import EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE\n",
    "from src.evals.eval_runner import test_correctness_with_visual_comparison\n",
    "from src.agents.multi_agent_pattern import run_multi_agent\n",
    "\n",
    "\n",
    "results = await test_correctness_with_visual_comparison(\n",
    "    call_agent=run_multi_agent,\n",
    "    eval_params=EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE,\n",
    "    n_results_search=20,\n",
    "    orchestrator_model=\"openai:gpt-4.1\",\n",
    "    researcher_model=\"openai:gpt-4.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.human_verified_searches import EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE\n",
    "from src.evals.eval_runner import test_correctness_with_visual_comparison\n",
    "from src.agents.multi_agent_pattern import run_multi_agent\n",
    "\n",
    "\n",
    "results = await test_correctness_with_visual_comparison(\n",
    "    call_agent=run_multi_agent,\n",
    "    eval_params=EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE,\n",
    "    n_results_search=20,\n",
    "    orchestrator_model=\"openai:gpt-4.1\",\n",
    "    researcher_model=\"openai:gpt-4.1-mini-2025-04-14\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.human_verified_searches import (\n",
    "    EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    "    EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE,\n",
    ")\n",
    "from src.evals.eval_runner import test_correctness_with_visual_comparison\n",
    "from src.agents.single_agent_pattern import run_single_agent\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "N_RESULTS_PER_SEARCH = 10\n",
    "\n",
    "models = [\n",
    "    \"openai:gpt-4.1\",\n",
    "    \"openai:gpt-4.1-mini-2025-04-14\",\n",
    "    \"openai:gpt-4.1-nano-2025-04-14\",\n",
    "    \"anthropic:claude-sonnet-4-20250514\",\n",
    "    \"anthropic:claude-3-7-sonnet-latest\",\n",
    "    \"anthropic:claude-3-5-sonnet-latest\",\n",
    "]\n",
    "\n",
    "single_agent_results = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Running model {model} for checkpoint {i}\")\n",
    "    # Check if model is in the latest checkpoint in ./checkpoints, skip if so\n",
    "\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    files = os.listdir(checkpoint_dir) if os.path.exists(checkpoint_dir) else []\n",
    "    nums = [\n",
    "        int(m.group(1))\n",
    "        for f in files\n",
    "        if (m := re.match(r\"single_agent_results_checkpoint_(\\d+)\\.csv\", f))\n",
    "    ]\n",
    "    if nums:\n",
    "        latest = max(nums)\n",
    "        df = pd.read_csv(\n",
    "            f\"{checkpoint_dir}/single_agent_results_checkpoint_{latest}.csv\"\n",
    "        )\n",
    "        if model in df[\"researcher_model\"].values:\n",
    "            print(f\"Skipping {model}, already in checkpoint {latest}\")\n",
    "            continue\n",
    "\n",
    "    # narrow precise task\n",
    "    narrow_task_results = await test_correctness_with_visual_comparison(\n",
    "        call_agent=run_single_agent,\n",
    "        eval_params=EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    "        n_results_search=N_RESULTS_PER_SEARCH,\n",
    "        researcher_model=model,\n",
    "    )\n",
    "    g_eval_narrow = (\n",
    "        narrow_task_results[\"eval_results\"].test_results[0].metrics_data[0].score\n",
    "        or None\n",
    "    )\n",
    "    single_agent_results.append(\n",
    "        {\n",
    "            \"agent_type\": \"single\",\n",
    "            \"researcher_model\": model,\n",
    "            \"orchestrator_model\": None,\n",
    "            \"task\": \"narrow\",\n",
    "            \"g_eval\": g_eval_narrow,\n",
    "            \"recall\": narrow_task_results[\"recall_matches\"],\n",
    "            \"total_extra_leads\": narrow_task_results[\"total_extra_leads\"],\n",
    "            \"n_results_per_search\": N_RESULTS_PER_SEARCH,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # broad less precise task\n",
    "    broad_task_results = await test_correctness_with_visual_comparison(\n",
    "        call_agent=run_single_agent,\n",
    "        eval_params=EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE,\n",
    "        n_results_search=10,\n",
    "        researcher_model=model,\n",
    "    )\n",
    "\n",
    "    g_eval_broad = (\n",
    "        broad_task_results[\"eval_results\"].test_results[0].metrics_data[0].score or None\n",
    "    )\n",
    "    single_agent_results.append(\n",
    "        {\n",
    "            \"agent_type\": \"single\",\n",
    "            \"researcher_model\": model,\n",
    "            \"orchestrator_model\": None,\n",
    "            \"task\": \"broad\",\n",
    "            \"g_eval\": g_eval_broad,\n",
    "            \"recall\": broad_task_results[\"recall_matches\"],\n",
    "            \"total_extra_leads\": broad_task_results[\"total_extra_leads\"],\n",
    "            \"n_results_per_search\": N_RESULTS_PER_SEARCH,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_single_agent = pd.DataFrame(single_agent_results)\n",
    "    df_single_agent.to_csv(\n",
    "        f\"./checkpoints/single_agent_results_checkpoint_{i}.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.DataFrame(single_agent_results)\n",
    "\n",
    "df.to_csv(\"single_agent_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style for better looking plots\n",
    "plt.style.use(\"default\")\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Recall comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Filter data for each task\n",
    "narrow_data = df[df[\"task\"] == \"narrow\"]\n",
    "broad_data = df[df[\"task\"] == \"broad\"]\n",
    "\n",
    "# Plot 1: Narrow Task Recall\n",
    "sns.barplot(data=narrow_data, x=\"main_model\", y=\"recall\", hue=\"agent_type\", ax=axes[0])\n",
    "axes[0].set_title(\"Recall - Narrow Task\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Main Model\", fontsize=12)\n",
    "axes[0].set_ylabel(\"Recall\", fontsize=12)\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "axes[0].legend(title=\"Agent Type\")\n",
    "\n",
    "# Plot 2: Broad Task Recall\n",
    "sns.barplot(data=broad_data, x=\"main_model\", y=\"recall\", hue=\"agent_type\", ax=axes[1])\n",
    "axes[1].set_title(\"Recall - Broad Task\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Main Model\", fontsize=12)\n",
    "axes[1].set_ylabel(\"Recall\", fontsize=12)\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1].legend(title=\"Agent Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G-Eval comparison plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Filter data for each task\n",
    "narrow_data = df[df[\"task\"] == \"narrow\"]\n",
    "broad_data = df[df[\"task\"] == \"broad\"]\n",
    "\n",
    "# Plot 1: Narrow Task G-Eval\n",
    "sns.barplot(data=narrow_data, x=\"main_model\", y=\"g_eval\", hue=\"agent_type\", ax=axes[0])\n",
    "axes[0].set_title(\"G-Eval Score - Narrow Task\", fontsize=14, fontweight=\"bold\")\n",
    "axes[0].set_xlabel(\"Main Model\", fontsize=12)\n",
    "axes[0].set_ylabel(\"G-Eval Score\", fontsize=12)\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "axes[0].legend(title=\"Agent Type\")\n",
    "\n",
    "# Plot 2: Broad Task G-Eval\n",
    "sns.barplot(data=broad_data, x=\"main_model\", y=\"g_eval\", hue=\"agent_type\", ax=axes[1])\n",
    "axes[1].set_title(\"G-Eval Score - Broad Task\", fontsize=14, fontweight=\"bold\")\n",
    "axes[1].set_xlabel(\"Main Model\", fontsize=12)\n",
    "axes[1].set_ylabel(\"G-Eval Score\", fontsize=12)\n",
    "axes[1].tick_params(axis=\"x\", rotation=45)\n",
    "axes[1].legend(title=\"Agent Type\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evals.human_verified_searches import (\n",
    "    EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    "    EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE,\n",
    ")\n",
    "from src.evals.eval_runner import test_correctness_with_visual_comparison\n",
    "from src.agents.single_agent_pattern import run_single_agent\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "N_RESULTS_PER_SEARCH = 10\n",
    "\n",
    "models = [\n",
    "    (\"openai:gpt-4.1\", \"openai:gpt-4.1\"),\n",
    "    (\"openai:gpt-4.1\", \"openai:gpt-4.1-mini-2025-04-14\"),\n",
    "    (\"openai:gpt-4.1-mini-2025-04-14\", \"gpt-4.1-nano-2025-04-14\"),\n",
    "    (\"anthropic:claude-sonnet-4-20250514\", \"anthropic:claude-3-7-sonnet-latest\"),\n",
    "    (\"anthropic:claude-3-7-sonnet-latest\", \"anthropic:claude-3-5-sonnet-latest\"),\n",
    "]\n",
    "\n",
    "multi_agent_results = []\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    # Check if model is in the latest checkpoint in ./checkpoints, skip if so\n",
    "\n",
    "    checkpoint_dir = \"./checkpoints\"\n",
    "    files = os.listdir(checkpoint_dir) if os.path.exists(checkpoint_dir) else []\n",
    "    nums = [\n",
    "        int(m.group(1))\n",
    "        for f in files\n",
    "        if (m := re.match(r\"multi_agent_results_checkpoint_(\\d+)\\.csv\", f))\n",
    "    ]\n",
    "    latest_df = None\n",
    "    if nums:\n",
    "        latest = max(nums)\n",
    "        checkpoint_path = (\n",
    "            f\"{checkpoint_dir}/multi_agent_results_checkpoint_{latest}.csv\"\n",
    "        )\n",
    "        latest_df = pd.read_csv(checkpoint_path)\n",
    "        # Check if this model combo is already in the latest checkpoint, skip if so\n",
    "        if (\n",
    "            (latest_df[\"orchestrator_model\"] == model[0])\n",
    "            & (latest_df[\"researcher_model\"] == model[1])\n",
    "        ).any():\n",
    "            print(f\"Skipping {model}, already in checkpoint {latest}\")\n",
    "            continue\n",
    "\n",
    "    # narrow precise task\n",
    "    narrow_task_results = await test_correctness_with_visual_comparison(\n",
    "        call_agent=run_single_agent,\n",
    "        eval_params=EDMONTON_HUMAN_NUTRITION_RESEARCH_UNIT__VERY_NARROW_SCOPE,\n",
    "        n_results_search=N_RESULTS_PER_SEARCH,\n",
    "        researcher_model=model[1],\n",
    "        orchestrator_model=model[0],\n",
    "    )\n",
    "    g_eval_narrow = (\n",
    "        narrow_task_results[\"eval_results\"].test_results[0].metrics_data[0].score\n",
    "        or None\n",
    "    )\n",
    "    multi_agent_results.append(\n",
    "        {\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"orchestrator_model\": model[0],\n",
    "            \"researcher_model\": model[1],\n",
    "            \"task\": \"narrow\",\n",
    "            \"g_eval\": g_eval_narrow,\n",
    "            \"recall\": narrow_task_results[\"recall_matches\"],\n",
    "            \"total_extra_leads\": narrow_task_results[\"total_extra_leads\"],\n",
    "            \"n_results_per_search\": N_RESULTS_PER_SEARCH,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # broad less precise task\n",
    "    broad_task_results = await test_correctness_with_visual_comparison(\n",
    "        call_agent=run_single_agent,\n",
    "        eval_params=EDMONTON_ALES_RESEARCHERS__MID_BROAD_SCOPE,\n",
    "        n_results_search=10,\n",
    "        researcher_model=model[1],\n",
    "        orchestrator_model=model[0],\n",
    "    )\n",
    "\n",
    "    g_eval_broad = (\n",
    "        broad_task_results[\"eval_results\"].test_results[0].metrics_data[0].score or None\n",
    "    )\n",
    "    multi_agent_results.append(\n",
    "        {\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"orchestrator_model\": model[0],\n",
    "            \"researcher_model\": model[1],\n",
    "            \"task\": \"broad\",\n",
    "            \"g_eval\": g_eval_broad,\n",
    "            \"recall\": broad_task_results[\"recall_matches\"],\n",
    "            \"total_extra_leads\": broad_task_results[\"total_extra_leads\"],\n",
    "            \"n_results_per_search\": N_RESULTS_PER_SEARCH,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Save the results to a csv file\n",
    "    df_multi_agent = pd.DataFrame(multi_agent_results)\n",
    "    df_multi_agent.to_csv(\n",
    "        f\"./checkpoints/multi_agent_results_checkpoint_{i}.csv\", index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_multi_agent = pd.DataFrame(multi_agent_results)\n",
    "\n",
    "df_multi_agent.to_csv(\"multi_agent_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_single_agent = pd.read_csv(\"./single_agent_results.csv\")\n",
    "df_multi_agent = pd.read_csv(\"./multi_agent_results.csv\")\n",
    "\n",
    "df = pd.concat([df_single_agent, df_multi_agent])\n",
    "df[\"main_model\"] = df.apply(\n",
    "    lambda row: f\"{row['orchestrator_model']} : {row['researcher_model']}\"\n",
    "    if pd.notnull(row[\"orchestrator_model\"])\n",
    "    else row[\"researcher_model\"],\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_recall_ranking_plots(df):\n",
    "    \"\"\"\n",
    "    Create a 2x2 grid of ordered point plots showing recall rankings for different agent types and tasks.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the evaluation results\n",
    "\n",
    "    Returns:\n",
    "    matplotlib.figure.Figure: The figure object containing the plots\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the figure and subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "    # Set overall style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Define the combinations for each subplot\n",
    "    plot_configs = [\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Narrow Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Narrow Task - Multi Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Broad Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Broad Task - Multi Agent\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for config in plot_configs:\n",
    "        # Filter data for this specific combination\n",
    "        subset = df[\n",
    "            (df[\"task\"] == config[\"task\"]) & (df[\"agent_type\"] == config[\"agent_type\"])\n",
    "        ]\n",
    "\n",
    "        if len(subset) == 0:\n",
    "            # Handle empty subset - show message\n",
    "            ax = axes[config[\"row\"], config[\"col\"]]\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No data available\\nfor {config['title']}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "            )\n",
    "            ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # Sort by recall (descending for ranking)\n",
    "        subset_sorted = subset.sort_values(\"recall\", ascending=False).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        # Create ranking (1 = best recall)\n",
    "        subset_sorted[\"rank\"] = range(1, len(subset_sorted) + 1)\n",
    "\n",
    "        # Create the ordered point plot\n",
    "        ax = axes[config[\"row\"], config[\"col\"]]\n",
    "\n",
    "        # Plot the points\n",
    "        ax.scatter(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"recall\"],\n",
    "            s=150,\n",
    "            alpha=0.8,\n",
    "            color=\"steelblue\",\n",
    "            edgecolors=\"darkblue\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        # Connect points with lines\n",
    "        ax.plot(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"recall\"],\n",
    "            color=\"steelblue\",\n",
    "            alpha=0.6,\n",
    "            linewidth=2,\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "        ax.set_xlabel(\"Rank (1 = Best Recall)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"Recall Score\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        # Set x-axis to show integer ranks\n",
    "        ax.set_xticks(subset_sorted[\"rank\"])\n",
    "        ax.set_xticklabels(subset_sorted[\"rank\"], fontsize=12)\n",
    "\n",
    "        # Format y-axis\n",
    "        ax.tick_params(axis=\"y\", labelsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        # Add value labels on points\n",
    "        for i, row in subset_sorted.iterrows():\n",
    "            ax.annotate(\n",
    "                f\"{row['recall']:.3f}\",\n",
    "                (row[\"rank\"], row[\"recall\"]),\n",
    "                xytext=(0, 15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "        # Add model name labels below x-axis\n",
    "        for i, row in subset_sorted.iterrows():\n",
    "            # Truncate long model names for better display\n",
    "            model_name = row[\"main_model\"]\n",
    "            if len(model_name) > 25:\n",
    "                model_name = model_name[:22] + \"...\"\n",
    "\n",
    "            ax.annotate(\n",
    "                model_name,\n",
    "                (row[\"rank\"], ax.get_ylim()[0]),\n",
    "                xytext=(0, -40),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"top\",\n",
    "                fontsize=10,\n",
    "                rotation=45,\n",
    "            )\n",
    "\n",
    "        # Adjust y-axis limits to accommodate labels\n",
    "        y_min, y_max = ax.get_ylim()\n",
    "        y_range = y_max - y_min\n",
    "        ax.set_ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(\n",
    "        \"Recall Performance Rankings by Agent Type and Task Complexity\",\n",
    "        fontsize=20,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.95,\n",
    "    )\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.92])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Example usage function\n",
    "def generate_recall_ranking_plots():\n",
    "    \"\"\"\n",
    "    Generate the recall ranking plots using the evaluation data.\n",
    "    Assumes the data files exist in the current directory.\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df_single_agent = pd.read_csv(\"./single_agent_results.csv\")\n",
    "    df_multi_agent = pd.read_csv(\"./multi_agent_results.csv\")\n",
    "\n",
    "    # Combine datasets\n",
    "    df = pd.concat([df_single_agent, df_multi_agent])\n",
    "\n",
    "    # Create main_model column\n",
    "    df[\"main_model\"] = df.apply(\n",
    "        lambda row: f\"{row['orchestrator_model']} : {row['researcher_model']}\"\n",
    "        if pd.notnull(row[\"orchestrator_model\"])\n",
    "        else row[\"researcher_model\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Generate the plots\n",
    "    fig = create_recall_ranking_plots(df)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = generate_recall_ranking_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_model_abbreviations(df):\n",
    "    \"\"\"Create shorter abbreviations for model names\"\"\"\n",
    "    model_mapping = {}\n",
    "\n",
    "    for model in df[\"main_model\"].unique():\n",
    "        if \" : \" in model:  # Multi-agent format\n",
    "            parts = model.split(\" : \")\n",
    "            orch = parts[0].split(\":\")[-1] if \":\" in parts[0] else parts[0]\n",
    "            res = parts[1].split(\":\")[-1] if \":\" in parts[1] else parts[1]\n",
    "\n",
    "            # Create short abbreviations\n",
    "            orch_short = (\n",
    "                orch.replace(\"gpt-4.1\", \"GPT4.1\")\n",
    "                .replace(\"claude-\", \"C-\")\n",
    "                .replace(\"sonnet\", \"S\")\n",
    "                .replace(\"anthropic\", \"A\")[:8]\n",
    "            )\n",
    "            res_short = (\n",
    "                res.replace(\"gpt-4.1\", \"GPT4.1\")\n",
    "                .replace(\"claude-\", \"C-\")\n",
    "                .replace(\"sonnet\", \"S\")\n",
    "                .replace(\"anthropic\", \"A\")[:8]\n",
    "            )\n",
    "\n",
    "            model_mapping[model] = f\"{orch_short}:{res_short}\"\n",
    "        else:  # Single agent format\n",
    "            short = model.split(\":\")[-1] if \":\" in model else model\n",
    "            short = (\n",
    "                short.replace(\"gpt-4.1\", \"GPT4.1\")\n",
    "                .replace(\"claude-\", \"C-\")\n",
    "                .replace(\"sonnet\", \"S\")\n",
    "                .replace(\"anthropic\", \"A\")\n",
    "            )\n",
    "            model_mapping[model] = short[:12]\n",
    "\n",
    "    return model_mapping\n",
    "\n",
    "\n",
    "def create_recall_ranking_plots_v2(df):\n",
    "    \"\"\"\n",
    "    Create a 2x2 grid of ordered point plots with better x-axis label handling.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create model abbreviations\n",
    "    model_mapping = create_model_abbreviations(df)\n",
    "\n",
    "    # Set up the figure and subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "    # Set overall style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Define the combinations for each subplot\n",
    "    plot_configs = [\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Narrow Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Narrow Task - Multi Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Broad Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Broad Task - Multi Agent\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Create a legend mapping for all subplots\n",
    "    legend_text = \"Model Abbreviations:\\n\" + \"\\n\".join(\n",
    "        [f\"{v}: {k}\" for k, v in model_mapping.items()]\n",
    "    )\n",
    "\n",
    "    for config in plot_configs:\n",
    "        # Filter data for this specific combination\n",
    "        subset = df[\n",
    "            (df[\"task\"] == config[\"task\"]) & (df[\"agent_type\"] == config[\"agent_type\"])\n",
    "        ]\n",
    "\n",
    "        if len(subset) == 0:\n",
    "            # Handle empty subset - show message\n",
    "            ax = axes[config[\"row\"], config[\"col\"]]\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No data available\\nfor {config['title']}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "            )\n",
    "            ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # Sort by recall (descending for ranking)\n",
    "        subset_sorted = subset.sort_values(\"recall\", ascending=False).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        # Create ranking (1 = best recall)\n",
    "        subset_sorted[\"rank\"] = range(1, len(subset_sorted) + 1)\n",
    "\n",
    "        # Create the ordered point plot\n",
    "        ax = axes[config[\"row\"], config[\"col\"]]\n",
    "\n",
    "        # Plot the points\n",
    "        ax.scatter(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"recall\"],\n",
    "            s=150,\n",
    "            alpha=0.8,\n",
    "            color=\"steelblue\",\n",
    "            edgecolors=\"darkblue\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        # Connect points with lines\n",
    "        ax.plot(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"recall\"],\n",
    "            color=\"steelblue\",\n",
    "            alpha=0.6,\n",
    "            linewidth=2,\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "        ax.set_xlabel(\"Rank (1 = Best Recall)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"Recall Score\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        # Set x-axis to show integer ranks\n",
    "        ax.set_xticks(subset_sorted[\"rank\"])\n",
    "\n",
    "        # Use abbreviated model names as x-axis labels\n",
    "        abbreviated_names = [\n",
    "            model_mapping[model] for model in subset_sorted[\"main_model\"]\n",
    "        ]\n",
    "        ax.set_xticklabels(abbreviated_names, fontsize=10, rotation=45, ha=\"right\")\n",
    "\n",
    "        # Format y-axis\n",
    "        ax.tick_params(axis=\"y\", labelsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        # Add value labels on points\n",
    "        for i, row in subset_sorted.iterrows():\n",
    "            ax.annotate(\n",
    "                f\"{row['recall']:.3f}\",\n",
    "                (row[\"rank\"], row[\"recall\"]),\n",
    "                xytext=(0, 15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(\n",
    "        \"Recall Performance Rankings by Agent Type and Task Complexity\",\n",
    "        fontsize=20,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.95,\n",
    "    )\n",
    "\n",
    "    # Add legend with model mappings in a text box\n",
    "    fig.text(\n",
    "        0.02,\n",
    "        0.02,\n",
    "        legend_text,\n",
    "        fontsize=8,\n",
    "        verticalalignment=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.15, 1, 0.92])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Alternative version with no x-axis labels, just numbers\n",
    "def create_recall_ranking_plots_v3(df):\n",
    "    \"\"\"\n",
    "    Create a 2x2 grid with numbered x-axis and a detailed legend table.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the figure and subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
    "\n",
    "    # Set overall style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Define the combinations for each subplot\n",
    "    plot_configs = [\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Narrow Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Narrow Task - Multi Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Broad Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Broad Task - Multi Agent\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Store legend information for each subplot\n",
    "    legend_info = {}\n",
    "\n",
    "    for config in plot_configs:\n",
    "        # Filter data for this specific combination\n",
    "        subset = df[\n",
    "            (df[\"task\"] == config[\"task\"]) & (df[\"agent_type\"] == config[\"agent_type\"])\n",
    "        ]\n",
    "\n",
    "        if len(subset) == 0:\n",
    "            # Handle empty subset - show message\n",
    "            ax = axes[config[\"row\"], config[\"col\"]]\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No data available\\nfor {config['title']}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "            )\n",
    "            ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # Sort by recall (descending for ranking)\n",
    "        subset_sorted = subset.sort_values(\"recall\", ascending=False).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        # Create ranking (1 = best recall)\n",
    "        subset_sorted[\"rank\"] = range(1, len(subset_sorted) + 1)\n",
    "\n",
    "        # Store legend info\n",
    "        legend_info[config[\"title\"]] = list(\n",
    "            zip(\n",
    "                subset_sorted[\"rank\"],\n",
    "                subset_sorted[\"main_model\"],\n",
    "                subset_sorted[\"recall\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the ordered point plot\n",
    "        ax = axes[config[\"row\"], config[\"col\"]]\n",
    "\n",
    "        # Plot the points\n",
    "        ax.scatter(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"recall\"],\n",
    "            s=150,\n",
    "            alpha=0.8,\n",
    "            color=\"steelblue\",\n",
    "            edgecolors=\"darkblue\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        # Connect points with lines\n",
    "        ax.plot(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"recall\"],\n",
    "            color=\"steelblue\",\n",
    "            alpha=0.6,\n",
    "            linewidth=2,\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "        ax.set_xlabel(\"Rank (1 = Best Recall)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"Recall Score\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        # Set x-axis to show integer ranks only\n",
    "        ax.set_xticks(subset_sorted[\"rank\"])\n",
    "        ax.set_xticklabels(subset_sorted[\"rank\"], fontsize=12)\n",
    "\n",
    "        # Format y-axis\n",
    "        ax.tick_params(axis=\"y\", labelsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        # Add value labels on points\n",
    "        for i, row in subset_sorted.iterrows():\n",
    "            ax.annotate(\n",
    "                f\"{row['recall']:.3f}\",\n",
    "                (row[\"rank\"], row[\"recall\"]),\n",
    "                xytext=(0, 15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(\n",
    "        \"Recall Performance Rankings by Agent Type and Task Complexity\",\n",
    "        fontsize=20,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.95,\n",
    "    )\n",
    "\n",
    "    # Create a detailed legend table\n",
    "    legend_text = \"MODEL RANKINGS:\\n\\n\"\n",
    "    for title, info in legend_info.items():\n",
    "        legend_text += f\"{title}:\\n\"\n",
    "        for rank, model, recall in info:\n",
    "            legend_text += f\"  {rank}. {model} (Recall: {recall:.3f})\\n\"\n",
    "        legend_text += \"\\n\"\n",
    "\n",
    "    # Add legend as text box\n",
    "    fig.text(\n",
    "        0.02,\n",
    "        0.02,\n",
    "        legend_text,\n",
    "        fontsize=9,\n",
    "        verticalalignment=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\", alpha=0.9),\n",
    "    )\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.3, 1, 0.92])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Updated main function with options\n",
    "def generate_recall_ranking_plots(version=\"v2\"):\n",
    "    \"\"\"\n",
    "    Generate the recall ranking plots using the evaluation data.\n",
    "\n",
    "    Parameters:\n",
    "    version (str): \"v2\" for abbreviated labels, \"v3\" for numbered ranks with legend\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df_single_agent = pd.read_csv(\"./single_agent_results.csv\")\n",
    "    df_multi_agent = pd.read_csv(\"./multi_agent_results.csv\")\n",
    "\n",
    "    # Combine datasets\n",
    "    df = pd.concat([df_single_agent, df_multi_agent])\n",
    "\n",
    "    # Create main_model column\n",
    "    df[\"main_model\"] = df.apply(\n",
    "        lambda row: f\"{row['orchestrator_model']} : {row['researcher_model']}\"\n",
    "        if pd.notnull(row[\"orchestrator_model\"])\n",
    "        else row[\"researcher_model\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Generate the plots based on version\n",
    "    if version == \"v3\":\n",
    "        fig = create_recall_ranking_plots_v3(df)\n",
    "    else:\n",
    "        fig = create_recall_ranking_plots_v2(df)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# For numbered ranks with legend version\n",
    "fig = generate_recall_ranking_plots(version=\"v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_model_abbreviations(df):\n",
    "    \"\"\"Create shorter abbreviations for model names\"\"\"\n",
    "    model_mapping = {}\n",
    "\n",
    "    for model in df[\"main_model\"].unique():\n",
    "        if \" : \" in model:  # Multi-agent format\n",
    "            parts = model.split(\" : \")\n",
    "            orch = parts[0].split(\":\")[-1] if \":\" in parts[0] else parts[0]\n",
    "            res = parts[1].split(\":\")[-1] if \":\" in parts[1] else parts[1]\n",
    "\n",
    "            # Create short abbreviations\n",
    "            orch_short = (\n",
    "                orch.replace(\"gpt-4.1\", \"GPT4.1\")\n",
    "                .replace(\"claude-\", \"C-\")\n",
    "                .replace(\"sonnet\", \"S\")\n",
    "                .replace(\"anthropic\", \"A\")[:8]\n",
    "            )\n",
    "            res_short = (\n",
    "                res.replace(\"gpt-4.1\", \"GPT4.1\")\n",
    "                .replace(\"claude-\", \"C-\")\n",
    "                .replace(\"sonnet\", \"S\")\n",
    "                .replace(\"anthropic\", \"A\")[:8]\n",
    "            )\n",
    "\n",
    "            model_mapping[model] = f\"{orch_short}:{res_short}\"\n",
    "        else:  # Single agent format\n",
    "            short = model.split(\":\")[-1] if \":\" in model else model\n",
    "            short = (\n",
    "                short.replace(\"gpt-4.1\", \"GPT4.1\")\n",
    "                .replace(\"claude-\", \"C-\")\n",
    "                .replace(\"sonnet\", \"S\")\n",
    "                .replace(\"anthropic\", \"A\")\n",
    "            )\n",
    "            model_mapping[model] = short[:12]\n",
    "\n",
    "    return model_mapping\n",
    "\n",
    "\n",
    "def create_geval_ranking_plots_v2(df):\n",
    "    \"\"\"\n",
    "    Create a 2x2 grid of ordered point plots with G-Eval rankings and abbreviated labels.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create model abbreviations\n",
    "    model_mapping = create_model_abbreviations(df)\n",
    "\n",
    "    # Set up the figure and subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "    # Set overall style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Define the combinations for each subplot\n",
    "    plot_configs = [\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Narrow Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Narrow Task - Multi Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Broad Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Broad Task - Multi Agent\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Create a legend mapping for all subplots\n",
    "    legend_text = \"Model Abbreviations:\\n\" + \"\\n\".join(\n",
    "        [f\"{v}: {k}\" for k, v in model_mapping.items()]\n",
    "    )\n",
    "\n",
    "    for config in plot_configs:\n",
    "        # Filter data for this specific combination\n",
    "        subset = df[\n",
    "            (df[\"task\"] == config[\"task\"]) & (df[\"agent_type\"] == config[\"agent_type\"])\n",
    "        ]\n",
    "\n",
    "        if len(subset) == 0:\n",
    "            # Handle empty subset - show message\n",
    "            ax = axes[config[\"row\"], config[\"col\"]]\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No data available\\nfor {config['title']}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "            )\n",
    "            ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # Sort by g_eval (descending for ranking) - handle NaN values\n",
    "        subset_clean = subset.dropna(subset=[\"g_eval\"])\n",
    "        if len(subset_clean) == 0:\n",
    "            ax = axes[config[\"row\"], config[\"col\"]]\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No G-Eval data available\\nfor {config['title']}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "            )\n",
    "            ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        subset_sorted = subset_clean.sort_values(\"g_eval\", ascending=False).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        # Create ranking (1 = best g_eval)\n",
    "        subset_sorted[\"rank\"] = range(1, len(subset_sorted) + 1)\n",
    "\n",
    "        # Create the ordered point plot\n",
    "        ax = axes[config[\"row\"], config[\"col\"]]\n",
    "\n",
    "        # Plot the points\n",
    "        ax.scatter(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"g_eval\"],\n",
    "            s=150,\n",
    "            alpha=0.8,\n",
    "            color=\"darkgreen\",\n",
    "            edgecolors=\"darkgreen\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        # Connect points with lines\n",
    "        ax.plot(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"g_eval\"],\n",
    "            color=\"darkgreen\",\n",
    "            alpha=0.6,\n",
    "            linewidth=2,\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "        ax.set_xlabel(\"Rank (1 = Best G-Eval Score)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"G-Eval Score\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        # Set x-axis to show integer ranks\n",
    "        ax.set_xticks(subset_sorted[\"rank\"])\n",
    "\n",
    "        # Use abbreviated model names as x-axis labels\n",
    "        abbreviated_names = [\n",
    "            model_mapping[model] for model in subset_sorted[\"main_model\"]\n",
    "        ]\n",
    "        ax.set_xticklabels(abbreviated_names, fontsize=10, rotation=45, ha=\"right\")\n",
    "\n",
    "        # Format y-axis\n",
    "        ax.tick_params(axis=\"y\", labelsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        # Add value labels on points\n",
    "        for i, row in subset_sorted.iterrows():\n",
    "            ax.annotate(\n",
    "                f\"{row['g_eval']:.3f}\",\n",
    "                (row[\"rank\"], row[\"g_eval\"]),\n",
    "                xytext=(0, 15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(\n",
    "        \"G-Eval Performance Rankings by Agent Type and Task Complexity\",\n",
    "        fontsize=20,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.95,\n",
    "    )\n",
    "\n",
    "    # Add legend with model mappings in a text box\n",
    "    fig.text(\n",
    "        0.02,\n",
    "        0.02,\n",
    "        legend_text,\n",
    "        fontsize=8,\n",
    "        verticalalignment=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.15, 1, 0.92])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_geval_ranking_plots_v3(df):\n",
    "    \"\"\"\n",
    "    Create a 2x2 grid with numbered x-axis and a detailed legend table for G-Eval scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set up the figure and subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(24, 16))\n",
    "\n",
    "    # Set overall style\n",
    "    plt.style.use(\"default\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Define the combinations for each subplot\n",
    "    plot_configs = [\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Narrow Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"narrow\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 0,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Narrow Task - Multi Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"single\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 0,\n",
    "            \"title\": \"Broad Task - Single Agent\",\n",
    "        },\n",
    "        {\n",
    "            \"task\": \"broad\",\n",
    "            \"agent_type\": \"multi\",\n",
    "            \"row\": 1,\n",
    "            \"col\": 1,\n",
    "            \"title\": \"Broad Task - Multi Agent\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # Store legend information for each subplot\n",
    "    legend_info = {}\n",
    "\n",
    "    for config in plot_configs:\n",
    "        # Filter data for this specific combination\n",
    "        subset = df[\n",
    "            (df[\"task\"] == config[\"task\"]) & (df[\"agent_type\"] == config[\"agent_type\"])\n",
    "        ]\n",
    "\n",
    "        if len(subset) == 0:\n",
    "            # Handle empty subset - show message\n",
    "            ax = axes[config[\"row\"], config[\"col\"]]\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No data available\\nfor {config['title']}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "            )\n",
    "            ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # Sort by g_eval (descending for ranking) - handle NaN values\n",
    "        subset_clean = subset.dropna(subset=[\"g_eval\"])\n",
    "        if len(subset_clean) == 0:\n",
    "            ax = axes[config[\"row\"], config[\"col\"]]\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No G-Eval data available\\nfor {config['title']}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=14,\n",
    "            )\n",
    "            ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            legend_info[config[\"title\"]] = []\n",
    "            continue\n",
    "\n",
    "        subset_sorted = subset_clean.sort_values(\"g_eval\", ascending=False).reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        # Create ranking (1 = best g_eval)\n",
    "        subset_sorted[\"rank\"] = range(1, len(subset_sorted) + 1)\n",
    "\n",
    "        # Store legend info\n",
    "        legend_info[config[\"title\"]] = list(\n",
    "            zip(\n",
    "                subset_sorted[\"rank\"],\n",
    "                subset_sorted[\"main_model\"],\n",
    "                subset_sorted[\"g_eval\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create the ordered point plot\n",
    "        ax = axes[config[\"row\"], config[\"col\"]]\n",
    "\n",
    "        # Plot the points\n",
    "        ax.scatter(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"g_eval\"],\n",
    "            s=150,\n",
    "            alpha=0.8,\n",
    "            color=\"darkgreen\",\n",
    "            edgecolors=\"darkgreen\",\n",
    "            linewidth=2,\n",
    "        )\n",
    "\n",
    "        # Connect points with lines\n",
    "        ax.plot(\n",
    "            subset_sorted[\"rank\"],\n",
    "            subset_sorted[\"g_eval\"],\n",
    "            color=\"darkgreen\",\n",
    "            alpha=0.6,\n",
    "            linewidth=2,\n",
    "            linestyle=\"-\",\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_title(config[\"title\"], fontsize=16, fontweight=\"bold\", pad=20)\n",
    "        ax.set_xlabel(\"Rank (1 = Best G-Eval Score)\", fontsize=14, fontweight=\"bold\")\n",
    "        ax.set_ylabel(\"G-Eval Score\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        # Set x-axis to show integer ranks only\n",
    "        ax.set_xticks(subset_sorted[\"rank\"])\n",
    "        ax.set_xticklabels(subset_sorted[\"rank\"], fontsize=12)\n",
    "\n",
    "        # Format y-axis\n",
    "        ax.tick_params(axis=\"y\", labelsize=12)\n",
    "        ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "\n",
    "        # Add value labels on points\n",
    "        for i, row in subset_sorted.iterrows():\n",
    "            ax.annotate(\n",
    "                f\"{row['g_eval']:.3f}\",\n",
    "                (row[\"rank\"], row[\"g_eval\"]),\n",
    "                xytext=(0, 15),\n",
    "                textcoords=\"offset points\",\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                fontsize=11,\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "    # Add overall title\n",
    "    fig.suptitle(\n",
    "        \"G-Eval Performance Rankings by Agent Type and Task Complexity\",\n",
    "        fontsize=20,\n",
    "        fontweight=\"bold\",\n",
    "        y=0.95,\n",
    "    )\n",
    "\n",
    "    # Create a detailed legend table\n",
    "    legend_text = \"G-EVAL MODEL RANKINGS:\\n\\n\"\n",
    "    for title, info in legend_info.items():\n",
    "        legend_text += f\"{title}:\\n\"\n",
    "        if len(info) == 0:\n",
    "            legend_text += \"  No data available\\n\"\n",
    "        else:\n",
    "            for rank, model, g_eval in info:\n",
    "                legend_text += f\"  {rank}. {model} (G-Eval: {g_eval:.3f})\\n\"\n",
    "        legend_text += \"\\n\"\n",
    "\n",
    "    # Add legend as text box\n",
    "    fig.text(\n",
    "        0.02,\n",
    "        0.02,\n",
    "        legend_text,\n",
    "        fontsize=9,\n",
    "        verticalalignment=\"bottom\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\", alpha=0.9),\n",
    "    )\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.3, 1, 0.92])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Main function for G-Eval plots\n",
    "def generate_geval_ranking_plots(version=\"v3\"):\n",
    "    \"\"\"\n",
    "    Generate the G-Eval ranking plots using the evaluation data.\n",
    "\n",
    "    Parameters:\n",
    "    version (str): \"v2\" for abbreviated labels, \"v3\" for numbered ranks with legend\n",
    "    \"\"\"\n",
    "    # Load the data\n",
    "    df_single_agent = pd.read_csv(\"./single_agent_results.csv\")\n",
    "    df_multi_agent = pd.read_csv(\"./multi_agent_results.csv\")\n",
    "\n",
    "    # Combine datasets\n",
    "    df = pd.concat([df_single_agent, df_multi_agent])\n",
    "\n",
    "    # Create main_model column\n",
    "    df[\"main_model\"] = df.apply(\n",
    "        lambda row: f\"{row['orchestrator_model']} : {row['researcher_model']}\"\n",
    "        if pd.notnull(row[\"orchestrator_model\"])\n",
    "        else row[\"researcher_model\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    # Generate the plots based on version\n",
    "    if version == \"v3\":\n",
    "        fig = create_geval_ranking_plots_v3(df)\n",
    "    else:\n",
    "        fig = create_geval_ranking_plots_v2(df)\n",
    "\n",
    "    # Show the plots\n",
    "    plt.show()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# For numbered ranks with legend version (recommended)\n",
    "fig = generate_geval_ranking_plots(version=\"v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
